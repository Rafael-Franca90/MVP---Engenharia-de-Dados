{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dfb07ec-bd0f-4dc9-9c58-18377370f83e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 01 - Pipeline Bronze (Ingestão de Dados Brutos)\n",
    "## Camada Bronze: Raw Data\n",
    "\n",
    "### O que é a Camada Bronze?\n",
    "A Camada Bronze é o primeiro estágio do nosso pipeline de dados. Pense nela como um \"armazém de dados brutos\" onde guardamos as informações exatamente como elas vêm das fontes originais, sem nenhuma modificação ou limpeza.\n",
    "\n",
    "### Por que precisamos dela?\n",
    "- **Backup histórico**: Se algo der errado nas próximas etapas, podemos sempre voltar aos dados originais\n",
    "- **Rastreabilidade**: Sabemos exatamente o que foi recebido da fonte e quando\n",
    "- **Flexibilidade**: Podemos reprocessar os dados de formas diferentes sem precisar baixá-los novamente\n",
    "\n",
    "### O que este notebook faz?\n",
    "1. **Busca dados de 2 APIs públicas na internet**: Uma com informações de países e outra com taxas de câmbio\n",
    "2. **Salva os dados em formato bruto**: JSON (formato de texto estruturado)\n",
    "3. **Adiciona informações de controle**: Data e hora em que os dados foram coletados\n",
    "4. **Organiza os dados por data**: Para facilitar a busca futura\n",
    "\n",
    "### APIs utilizadas:\n",
    "- **REST Countries API**: Dados de 195 países (população, área, capital, idiomas, etc.)\n",
    "- **Exchange Rate API**: Taxas de câmbio (quanto vale cada moeda em relação ao dólar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cff1f389-d8ac-470b-ab4b-3f314521030b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Preparação do Ambiente\n",
    "\n",
    "### O que vai acontecer aqui?\n",
    "Antes de começar a trabalhar, precisamos preparar nosso ambiente de trabalho. É como arrumar uma bancada antes de cozinhar: pegamos todos os utensílios e ingredientes que vamos usar.\n",
    "\n",
    "### O que vamos preparar?\n",
    "1. **Bibliotecas**: Ferramentas prontas que nos ajudam a fazer tarefas específicas (ex: baixar dados da internet, trabalhar com datas)\n",
    "2. **Endereços das APIs**: URLs (links) de onde vamos buscar os dados\n",
    "3. **Configurações de segurança**: Tempo máximo de espera, número de tentativas se algo falhar\n",
    "4. **Nomes das tabelas**: Onde vamos salvar os dados no banco de dados\n",
    "5. **Funções auxiliares**: \"Receitas\" de código que vamos reutilizar várias vezes\n",
    "\n",
    "### Por que isso é importante?\n",
    "Se organizarmos bem essa etapa, o resto do processo fica muito mais fácil e podemos reutilizar esse código no futuro.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af554635-29cc-4804-8892-cee43a0c9489",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configurações e funções carregadas com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Importações\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime, date\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "\n",
    "# Configurações (inline para evitar problemas com %run)\n",
    "API_URLS = {\n",
    "    'countries': 'https://restcountries.com/v3.1/independent?status=true',\n",
    "    'exchange_rates': 'https://api.exchangerate-api.com/v4/latest/USD'\n",
    "}\n",
    "\n",
    "REQUEST_CONFIG = {\n",
    "    'timeout': 30,\n",
    "    'max_retries': 3,\n",
    "    'retry_delay': 5\n",
    "}\n",
    "\n",
    "SCHEMAS = {\n",
    "    'bronze': 'workspace.bronze',\n",
    "    'silver': 'workspace.silver',\n",
    "    'gold': 'workspace.gold'\n",
    "}\n",
    "\n",
    "BRONZE_TABLES = {\n",
    "    'countries': f\"{SCHEMAS['bronze']}.countries_raw\",\n",
    "    'exchange_rates': f\"{SCHEMAS['bronze']}.exchange_rates_raw\"\n",
    "}\n",
    "\n",
    "# Funções utilitárias\n",
    "def fetch_api_data(url, timeout=30, max_retries=3):\n",
    "    \"\"\"Faz requisição HTTP com retry logic\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=timeout)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Tentativa {attempt + 1}/{max_retries} falhou: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(REQUEST_CONFIG['retry_delay'])\n",
    "            else:\n",
    "                raise Exception(f\"Falha ao acessar API após {max_retries} tentativas: {str(e)}\")\n",
    "\n",
    "def get_execution_metadata():\n",
    "    \"\"\"Retorna metadados de execução com ID único por execução\"\"\"\n",
    "    now = datetime.now()\n",
    "    return {\n",
    "        'ingestion_timestamp': now,\n",
    "        'execution_date': now.date(),\n",
    "        'execution_id': now.strftime('%Y%m%d_%H%M%S')  # ID único: YYYYMMDD_HHMMSS\n",
    "    }\n",
    "\n",
    "def log_metrics(stage, table_name, record_count, execution_time=None):\n",
    "    \"\"\"Loga métricas de execução\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MÉTRICAS - {stage.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Tabela: {table_name}\")\n",
    "    print(f\"Registros: {record_count:,}\")\n",
    "    if execution_time:\n",
    "        print(f\"Tempo de execução: {execution_time:.2f} segundos\")\n",
    "    print(f\"Timestamp: {datetime.now()}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "def create_database_if_not_exists(spark, schema_name):\n",
    "    \"\"\"Cria database se não existir\"\"\"\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {schema_name}\")\n",
    "    print(f\"Database '{schema_name}' verificado/criado com sucesso\")\n",
    "\n",
    "def optimize_table(spark, table_name, zorder_columns=None):\n",
    "    \"\"\"Otimiza tabela Delta com OPTIMIZE e Z-ORDER\"\"\"\n",
    "    print(f\"Otimizando tabela {table_name}...\")\n",
    "    if zorder_columns:\n",
    "        zorder_cols = ', '.join(zorder_columns)\n",
    "        spark.sql(f\"OPTIMIZE {table_name} ZORDER BY ({zorder_cols})\")\n",
    "    else:\n",
    "        spark.sql(f\"OPTIMIZE {table_name}\")\n",
    "    print(f\"Tabela {table_name} otimizada com sucesso\")\n",
    "\n",
    "print(\"✓ Configurações e funções carregadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c179f64b-e557-4331-af47-8804a15cf2b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Iniciando o Processo\n",
    "\n",
    "**O que este código faz?**\n",
    "- Marca o momento exato em que o pipeline começou a rodar\n",
    "- Mostra na tela informações sobre quando o processo iniciou\n",
    "\n",
    "**Por que fazer isso?**\n",
    "Isso nos ajuda a medir quanto tempo o processo levou e a identificar quando os dados foram coletados. Se algo der errado, sabemos exatamente em que momento aconteceu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d22e3b4-c02b-4020-972d-03b40bdb139b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Bronze - Iniciado\nTimestamp: 2025-12-07 13:04:48.931962\nData de Execução: 2025-12-07\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Pipeline Bronze - Iniciado\")\n",
    "print(f\"Timestamp: {datetime.now()}\")\n",
    "print(f\"Data de Execução: {date.today()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f94915f-1d6c-4c9a-ab9a-54a54e97abbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Criando o \"Armazém\" Bronze\n",
    "\n",
    "### O que é um schema/database?\n",
    "Pense em um schema como uma **pasta no computador**. Dentro dele, vamos criar várias \"planilhas\" (tabelas) com nossos dados. Estamos criando uma pasta chamada `workspace.bronze` para guardar os dados brutos.\n",
    "\n",
    "### Por que verificar se já existe?\n",
    "Se esta pasta já existe de uma execução anterior, não queremos criar outra. É como verificar se uma pasta já existe antes de tentar criá-la no Windows.\n",
    "\n",
    "### O que acontece aqui?\n",
    "O código cria a pasta `bronze` se ela não existir ainda. Se já existir, apenas confirma que ela está lá.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5ce3f7f-624d-4d68-bc25-5144f953eb06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database 'workspace.bronze' verificado/criado com sucesso\n"
     ]
    }
   ],
   "source": [
    "# Criar schema bronze se não existir\n",
    "create_database_if_not_exists(spark, SCHEMAS['bronze'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aab7b91-bddc-4789-806c-233b2bd578f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Coletando Dados dos Países\n",
    "\n",
    "### O que é uma API?\n",
    "Uma API é como um **garçom em um restaurante**. Você faz um pedido (requisição), ele vai até a cozinha (servidor) e traz sua comida (dados). No nosso caso, estamos pedindo informações sobre países para a REST Countries API.\n",
    "\n",
    "### Passo 1: Fazendo o Pedido à API\n",
    "\n",
    "**O que este código faz?**\n",
    "1. Acessa o endereço da API na internet (como abrir um site no navegador)\n",
    "2. Faz o pedido: \"Me dê informações sobre todos os países independentes\"\n",
    "3. Espera a resposta (até 30 segundos)\n",
    "4. Se der erro (internet caiu, servidor lento), tenta novamente até 3 vezes\n",
    "5. Recebe os dados em formato JSON (como um grande dicionário de informações)\n",
    "\n",
    "**O que esperamos receber?**\n",
    "Informações de aproximadamente **195 países por execução**, incluindo:\n",
    "- Nome oficial e comum\n",
    "- Capital\n",
    "- População\n",
    "- Área territorial\n",
    "- Idiomas falados\n",
    "- Moedas utilizadas\n",
    "- Coordenadas geográficas\n",
    "- E muito mais!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "480ef33e-34a7-4f11-8140-3013032967e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n============================================================\nINGESTÃO: REST COUNTRIES API\n============================================================\n✓ Dados recebidos: 195 países\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INGESTÃO: REST COUNTRIES API\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Fazer requisição à API\n",
    "countries_data = fetch_api_data(\n",
    "    url=API_URLS['countries'],\n",
    "    timeout=REQUEST_CONFIG['timeout'],\n",
    "    max_retries=REQUEST_CONFIG['max_retries']\n",
    ")\n",
    "\n",
    "print(f\"✓ Dados recebidos: {len(countries_data)} países\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c1f3c5a-b44f-4a62-840b-aac59ad37326",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Passo 2: Preparando os Dados para Salvar\n",
    "\n",
    "**O que está acontecendo aqui?**\n",
    "Agora que temos os dados dos países, precisamos **adicionar informações de controle** antes de salvá-los. É como carimbar um documento com a data de recebimento.\n",
    "\n",
    "**Informações que estamos adicionando:**\n",
    "1. **data**: O JSON completo do país (todas as informações) convertido para texto\n",
    "2. **ingestion_timestamp**: Data e hora exata em que coletamos esses dados (ex: 2025-12-05 14:32:15)\n",
    "3. **data_source**: De onde vieram os dados (rest_countries_api)\n",
    "4. **execution_date**: Apenas a data (ex: 2025-12-05) - usada para organizar os dados por dia\n",
    "\n",
    "**Por que converter JSON para texto (string)?**\n",
    "Na camada Bronze, queremos manter os dados **exatamente** como vieram da API. Guardar como texto garante que nada seja modificado ou perdido. Nas próximas etapas, vamos \"abrir\" esse texto e extrair as informações que precisamos.\n",
    "\n",
    "**Analogia:**\n",
    "É como receber 195 cartas lacradas (cada país = uma carta). Em vez de abrir e organizar o conteúdo agora, estamos apenas:\n",
    "- Guardando as cartas lacradas\n",
    "- Anotando quando recebemos cada uma\n",
    "- Marcando de onde vieram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f100b393-b8ff-41c6-a4bd-eaf8fe021718",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Registros preparados: 195\n"
     ]
    }
   ],
   "source": [
    "# Obter metadados de execução\n",
    "metadata = get_execution_metadata()\n",
    "\n",
    "# Preparar dados com metadados\n",
    "countries_records = []\n",
    "for country in countries_data:\n",
    "    record = {\n",
    "        'data': json.dumps(country),  # JSON como string\n",
    "        'ingestion_timestamp': metadata['ingestion_timestamp'],\n",
    "        'data_source': 'rest_countries_api',\n",
    "        'execution_date': metadata['execution_date']\n",
    "    }\n",
    "    countries_records.append(record)\n",
    "\n",
    "print(f\"✓ Registros preparados: {len(countries_records)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "036edb25-f941-4461-aeea-1d8d00cb6727",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Passo 3: Transformando em uma \"Planilha\" (DataFrame)\n",
    "\n",
    "**O que é um DataFrame?**\n",
    "Um DataFrame é como uma **planilha do Excel** em memória, com linhas e colunas. Cada linha é um país, e cada coluna é um tipo de informação (data, timestamp, fonte, etc.).\n",
    "\n",
    "**O que estamos fazendo aqui?**\n",
    "1. **Definindo a estrutura da planilha** (schema): Quais colunas teremos e que tipo de dado cada uma vai guardar\n",
    "   - `data`: Texto (STRING) - obrigatório (NOT NULL)\n",
    "   - `ingestion_timestamp`: Data e hora (TIMESTAMP) - obrigatório\n",
    "   - `data_source`: Texto (STRING) - obrigatório\n",
    "   - `execution_date`: Data (DATE) - obrigatório\n",
    "\n",
    "2. **Criando a planilha**: Pegamos os 195 registros que preparamos e colocamos nessa estrutura\n",
    "\n",
    "3. **Mostrando uma amostra**: Exibimos 3 linhas na tela para confirmar que está tudo certo\n",
    "\n",
    "**Por que definir tipos de dados?**\n",
    "Se dissermos que uma coluna é de data, o banco de dados sabe fazer operações específicas com ela (ex: filtrar por mês, calcular diferenças de dias). Se deixarmos tudo como texto, perdemos essas facilidades.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c82c3f4d-2117-4969-b0be-a56c1b676dd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nAmostra dos dados:\n+----------------------------------------------------------------------------------------------------+--------------------------+------------------+--------------+\n|data_sample                                                                                         |ingestion_timestamp       |data_source       |execution_date|\n+----------------------------------------------------------------------------------------------------+--------------------------+------------------+--------------+\n|{\"name\": {\"common\": \"Antigua and Barbuda\", \"official\": \"Antigua and Barbuda\", \"nativeName\": {\"eng\": |2025-12-07 13:04:51.165756|rest_countries_api|2025-12-07    |\n|{\"name\": {\"common\": \"Bhutan\", \"official\": \"Kingdom of Bhutan\", \"nativeName\": {\"dzo\": {\"official\": \"\\|2025-12-07 13:04:51.165756|rest_countries_api|2025-12-07    |\n|{\"name\": {\"common\": \"Italy\", \"official\": \"Italian Republic\", \"nativeName\": {\"ita\": {\"official\": \"Rep|2025-12-07 13:04:51.165756|rest_countries_api|2025-12-07    |\n+----------------------------------------------------------------------------------------------------+--------------------------+------------------+--------------+\nonly showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "# Criar DataFrame\n",
    "schema = StructType([\n",
    "    StructField('data', StringType(), False),\n",
    "    StructField('ingestion_timestamp', TimestampType(), False),\n",
    "    StructField('data_source', StringType(), False),\n",
    "    StructField('execution_date', DateType(), False)\n",
    "])\n",
    "\n",
    "df_countries_bronze = spark.createDataFrame(countries_records, schema=schema)\n",
    "\n",
    "# Visualizar amostra\n",
    "print(\"\\nAmostra dos dados:\")\n",
    "df_countries_bronze.select(\n",
    "    substring(col('data'), 1, 100).alias('data_sample'),\n",
    "    'ingestion_timestamp',\n",
    "    'data_source',\n",
    "    'execution_date'\n",
    ").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d12e70a3-6127-4bc3-9ca4-514cd0a42a13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Passo 4: Salvando no Banco de Dados (Delta Lake)\n",
    "\n",
    "**O que é Delta Lake?**\n",
    "Delta Lake é um **formato moderno de armazenamento de dados** que funciona como um banco de dados, mas é otimizado para grandes volumes e oferece recursos avançados como:\n",
    "- \"Time travel\" (ver como os dados estavam no passado)\n",
    "- Transações ACID (garantia de consistência)\n",
    "- Performance otimizada para análises\n",
    "\n",
    "**O que este código faz?**\n",
    "1. **Pega a planilha** (DataFrame) que criamos\n",
    "2. **Salva em formato Delta Lake**\n",
    "3. **Modo append**: Adiciona os novos dados sem apagar os antigos (como adicionar linhas no final de uma planilha)\n",
    "4. **Particionamento por data**: Organiza os dados em \"pastas\" separadas por dia\n",
    "   - Exemplo: pasta `execution_date=2025-12-05/` vai ter os dados de hoje\n",
    "   - pasta `execution_date=2025-12-06/` terá os dados de amanhã\n",
    "5. **Cria uma tabela**: Registra a tabela no catálogo do Databricks com o nome `workspace.bronze.countries_raw`\n",
    "\n",
    "**Por que particionar por data?**\n",
    "Imagine que você tem 1 ano de dados (365 dias). Se precisar consultar só os dados de ontem, o sistema vai direto na \"pasta\" de ontem, sem precisar vasculhar os 365 dias. Isso deixa as consultas **muito mais rápidas**.\n",
    "\n",
    "**Analogia:**\n",
    "É como organizar documentos em pastas por mês. Se você precisa de algo de dezembro, vai direto na pasta de dezembro, em vez de procurar em um monte de papel solto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06bd1f3d-3449-48c4-9040-874637655a7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n✓ Dados salvos em: workspace.bronze.countries_raw\n"
     ]
    }
   ],
   "source": [
    "# Salvar em Delta Lake (modo append, particionado por execution_date)\n",
    "(\n",
    "    df_countries_bronze\n",
    "    .write\n",
    "    .format('delta')\n",
    "    .mode('append')\n",
    "    .partitionBy('execution_date')\n",
    "    .saveAsTable(BRONZE_TABLES['countries'])\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Dados salvos em: {BRONZE_TABLES['countries']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0820772-7f70-41ef-91d2-15eabfea66b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registrando documentação no Unity Catalog...\n✓ Documentação Unity Catalog aplicada: countries_raw\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# DOCUMENTAÇÃO UNITY CATALOG - countries_raw\n",
    "# =====================================================\n",
    "# Registrar metadados da tabela e colunas no Unity Catalog\n",
    "\n",
    "print(\"Registrando documentação no Unity Catalog...\")\n",
    "\n",
    "# Propriedades da tabela\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE workspace.bronze.countries_raw \n",
    "SET TBLPROPERTIES (\n",
    "    'comment' = 'Dados brutos de paises coletados da REST Countries API. Armazena snapshot completo de 195 paises independentes por execucao em formato JSON.',\n",
    "    'source' = 'REST Countries API v3.1',\n",
    "    'source_url' = 'https://restcountries.com/v3.1/independent',\n",
    "    'layer' = 'bronze',\n",
    "    'update_frequency' = 'daily',\n",
    "    'retention_days' = '90'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Comentários das colunas\n",
    "spark.sql(\"ALTER TABLE workspace.bronze.countries_raw ALTER COLUMN data COMMENT 'JSON bruto completo retornado pela API com todos os atributos do pais (nome, populacao, area, moedas, idiomas, etc)'\")\n",
    "spark.sql(\"ALTER TABLE workspace.bronze.countries_raw ALTER COLUMN ingestion_timestamp COMMENT 'Timestamp UTC exato do momento da ingestao dos dados no pipeline'\")\n",
    "spark.sql(\"ALTER TABLE workspace.bronze.countries_raw ALTER COLUMN data_source COMMENT 'Identificador da fonte de dados: rest_countries_api'\")\n",
    "spark.sql(\"ALTER TABLE workspace.bronze.countries_raw ALTER COLUMN execution_date COMMENT 'Data de execucao do pipeline (YYYY-MM-DD), usada como chave de particionamento'\")\n",
    "\n",
    "print(\"✓ Documentação Unity Catalog aplicada: countries_raw\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ecacbf9-d588-485a-b10c-b32157e62529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Passo 5: Verificando se Deu Tudo Certo\n",
    "\n",
    "**O que este código faz?**\n",
    "Depois de salvar os dados, precisamos **confirmar que realmente foram salvos corretamente**. É como conferir se um arquivo foi salvo no computador depois de clicar em \"Salvar\".\n",
    "\n",
    "**Passos da verificação:**\n",
    "1. **Lê a tabela que acabamos de criar** do banco de dados\n",
    "2. **Conta quantas linhas tem** (esperamos **195 por execução**, uma para cada país)\n",
    "3. **Mostra um resumo** com métricas:\n",
    "   - Nome da tabela\n",
    "   - Número de registros salvos\n",
    "   - Data e hora da verificação\n",
    "\n",
    "**Por que fazer isso?**\n",
    "Às vezes algo pode dar errado no meio do caminho (memória insuficiente, erro de rede, etc.) e os dados não serem salvos completamente. Essa verificação garante que **todos os 195 países por execução** foram salvos com sucesso.\n",
    "\n",
    "**O que significa \"count\"?**\n",
    "É literalmente contar quantas linhas existem na tabela. Se salvamos 195 países, devemos ter 195 linhas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19ce1454-f1fc-478a-b344-76d1c045d8ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n============================================================\nMÉTRICAS - BRONZE\n============================================================\nTabela: workspace.bronze.countries_raw\nRegistros: 975\nTimestamp: 2025-12-07 13:05:00.732936\n============================================================\n\n"
     ]
    }
   ],
   "source": [
    "# Verificar dados salvos\n",
    "df_verify = spark.table(BRONZE_TABLES['countries'])\n",
    "count = df_verify.count()\n",
    "\n",
    "log_metrics(\n",
    "    stage='bronze',\n",
    "    table_name=BRONZE_TABLES['countries'],\n",
    "    record_count=count\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "496b8d2b-1050-4cc4-a23a-aeae1719e382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Coletando Taxas de Câmbio\n",
    "\n",
    "### O que são taxas de câmbio?\n",
    "Taxas de câmbio dizem **quanto vale cada moeda em relação ao dólar americano (USD)**. Por exemplo:\n",
    "- 1 USD = 5,20 BRL (reais brasileiros)\n",
    "- 1 USD = 0,92 EUR (euros)\n",
    "- 1 USD = 150 JPY (ienes japoneses)\n",
    "\n",
    "### Por que coletar essas taxas?\n",
    "Vários países usam moedas diferentes. Ter as taxas de câmbio nos permite comparar economias e entender o poder de compra em cada país.\n",
    "\n",
    "### Passo 1: Fazendo o Pedido à API\n",
    "\n",
    "**O que este código faz?**\n",
    "1. Acessa a Exchange Rate API na internet\n",
    "2. Pede: \"Me dê as taxas de câmbio atuais tendo o dólar (USD) como base\"\n",
    "3. Espera a resposta (até 30 segundos)\n",
    "4. Se der erro, tenta novamente até 3 vezes\n",
    "5. Recebe um JSON com todas as taxas (aproximadamente 160 moedas)\n",
    "\n",
    "**Exemplo do que recebemos:**\n",
    "```json\n",
    "{\n",
    "  \"base\": \"USD\",\n",
    "  \"date\": \"2025-12-05\",\n",
    "  \"rates\": {\n",
    "    \"BRL\": 5.20,\n",
    "    \"EUR\": 0.92,\n",
    "    \"GBP\": 0.79,\n",
    "    \"JPY\": 150.32,\n",
    "    ...\n",
    "  }\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21815b1c-00d5-44a7-a8d6-1249e86215f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n============================================================\nINGESTÃO: EXCHANGE RATE API\n============================================================\n✓ Dados recebidos: 166 taxas de câmbio\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INGESTÃO: EXCHANGE RATE API\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Fazer requisição à API\n",
    "exchange_data = fetch_api_data(\n",
    "    url=API_URLS['exchange_rates'],\n",
    "    timeout=REQUEST_CONFIG['timeout'],\n",
    "    max_retries=REQUEST_CONFIG['max_retries']\n",
    ")\n",
    "\n",
    "print(f\"✓ Dados recebidos: {len(exchange_data.get('rates', {}))} taxas de câmbio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a543bd21-7546-467e-9b0c-f662cd266308",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Passo 2: Preparando e Criando a Planilha\n",
    "\n",
    "**O que está acontecendo aqui?**\n",
    "Assim como fizemos com os países, agora vamos:\n",
    "\n",
    "1. **Adicionar metadados de controle**:\n",
    "   - Converter o JSON completo para texto\n",
    "   - Marcar data e hora da coleta\n",
    "   - Identificar a fonte (exchange_rate_api)\n",
    "   - Registrar a data de execução\n",
    "\n",
    "2. **Criar um DataFrame** (planilha) com a mesma estrutura de antes:\n",
    "   - Coluna `data`: JSON completo como texto\n",
    "   - Coluna `ingestion_timestamp`: Quando coletamos\n",
    "   - Coluna `data_source`: De onde veio\n",
    "   - Coluna `execution_date`: Data para particionamento\n",
    "\n",
    "3. **Mostrar uma amostra** dos primeiros 150 caracteres do JSON\n",
    "\n",
    "**Diferença em relação aos países:**\n",
    "Com países, tínhamos 195 registros (uma linha para cada país). Com taxas de câmbio, temos **apenas 1 registro** contendo TODAS as taxas dentro do JSON. É uma diferença de modelagem: um único arquivo com todas as conversões.\n",
    "\n",
    "**Analogia:**\n",
    "- Países: 195 cartas lacradas (uma para cada país)\n",
    "- Taxas de câmbio: 1 carta lacrada contendo uma tabela com 160 moedas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b1c8099-6ee4-48a1-b349-fd75f5f97045",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nAmostra dos dados:\n+------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------+-----------------+\n|data_sample                                                                                                                                           |ingestion_timestamp       |data_source      |\n+------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------+-----------------+\n|{\"provider\": \"https://www.exchangerate-api.com\", \"WARNING_UPGRADE_TO_V6\": \"https://www.exchangerate-api.com/docs/free\", \"terms\": \"https://www.exchange|2025-12-07 13:05:01.607922|exchange_rate_api|\n+------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Preparar dados com metadados\n",
    "metadata = get_execution_metadata()\n",
    "\n",
    "exchange_record = [{\n",
    "    'data': json.dumps(exchange_data),\n",
    "    'ingestion_timestamp': metadata['ingestion_timestamp'],\n",
    "    'data_source': 'exchange_rate_api',\n",
    "    'execution_date': metadata['execution_date']\n",
    "}]\n",
    "\n",
    "# Criar DataFrame\n",
    "df_exchange_bronze = spark.createDataFrame(exchange_record, schema=schema)\n",
    "\n",
    "print(\"\\nAmostra dos dados:\")\n",
    "df_exchange_bronze.select(\n",
    "    substring(col('data'), 1, 150).alias('data_sample'),\n",
    "    'ingestion_timestamp',\n",
    "    'data_source'\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bc8026c-b181-41db-b93a-52eb430934ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Passo 3: Salvando as Taxas de Câmbio\n",
    "\n",
    "**O que este código faz?**\n",
    "Mesma operação que fizemos com os países:\n",
    "1. Pega o DataFrame (planilha) com as taxas de câmbio\n",
    "2. Salva em formato Delta Lake\n",
    "3. Modo append: Adiciona aos dados existentes sem apagar\n",
    "4. Particiona por data: Organiza em \"pastas\" por dia de coleta\n",
    "5. Cria a tabela no catálogo: `workspace.bronze.exchange_rates_raw`\n",
    "\n",
    "**Por que particionar se temos só 1 registro?**\n",
    "Hoje temos 1 registro. Mas se rodarmos esse pipeline todos os dias por 1 ano, teremos 365 registros (um para cada dia). O particionamento permite consultar rapidamente \"qual era a taxa de câmbio em 15 de março?\" sem vasculhar todos os 365 dias.\n",
    "\n",
    "**Uso futuro:**\n",
    "Essas taxas podem mudar diariamente. Guardando o histórico particionado, podemos fazer análises como:\n",
    "- \"Como o real se valorizou/desvalorizou nos últimos 6 meses?\"\n",
    "- \"Qual foi a maior taxa EUR/USD do ano?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc9e6f92-f6ba-4372-9bb5-1ac01580b810",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n✓ Dados salvos em: workspace.bronze.exchange_rates_raw\n"
     ]
    }
   ],
   "source": [
    "# Salvar em Delta Lake\n",
    "(\n",
    "    df_exchange_bronze\n",
    "    .write\n",
    "    .format('delta')\n",
    "    .mode('append')\n",
    "    .partitionBy('execution_date')\n",
    "    .saveAsTable(BRONZE_TABLES['exchange_rates'])\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Dados salvos em: {BRONZE_TABLES['exchange_rates']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8a1a749-92b1-4adc-a2a2-93222b6500c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registrando documentação no Unity Catalog...\n✓ Documentação Unity Catalog aplicada: exchange_rates_raw\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# DOCUMENTAÇÃO UNITY CATALOG - exchange_rates_raw\n",
    "# =====================================================\n",
    "# Registrar metadados da tabela e colunas no Unity Catalog\n",
    "\n",
    "print(\"Registrando documentação no Unity Catalog...\")\n",
    "\n",
    "# Propriedades da tabela\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE workspace.bronze.exchange_rates_raw \n",
    "SET TBLPROPERTIES (\n",
    "    'comment' = 'Taxas de cambio diarias em relacao ao USD coletadas da Exchange Rate API. Um registro por execucao contendo todas as moedas (~160 taxas).',\n",
    "    'source' = 'Exchange Rate API',\n",
    "    'source_url' = 'https://api.exchangerate-api.com/v4/latest/USD',\n",
    "    'layer' = 'bronze',\n",
    "    'update_frequency' = 'daily',\n",
    "    'retention_days' = '90'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Comentários das colunas\n",
    "spark.sql(\"ALTER TABLE workspace.bronze.exchange_rates_raw ALTER COLUMN data COMMENT 'JSON contendo taxas de cambio: {base: USD, date: YYYY-MM-DD, rates: {moeda: valor}}'\")\n",
    "spark.sql(\"ALTER TABLE workspace.bronze.exchange_rates_raw ALTER COLUMN ingestion_timestamp COMMENT 'Timestamp UTC do momento da ingestao dos dados no pipeline'\")\n",
    "spark.sql(\"ALTER TABLE workspace.bronze.exchange_rates_raw ALTER COLUMN data_source COMMENT 'Identificador da fonte de dados: exchange_rate_api'\")\n",
    "spark.sql(\"ALTER TABLE workspace.bronze.exchange_rates_raw ALTER COLUMN execution_date COMMENT 'Data de execucao do pipeline (YYYY-MM-DD), usada como chave de particionamento'\")\n",
    "\n",
    "print(\"✓ Documentação Unity Catalog aplicada: exchange_rates_raw\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8765c8a5-c7c1-4f5f-94c7-cfec3c727e45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⚠️ Nota sobre a tabela exchange_rates_raw**\n",
    "\n",
    "Esta tabela foi criada para demonstrar **ingestão multi-fonte** (coleta de dados de múltiplas APIs), uma boa prática em engenharia de dados. No entanto, os dados de taxas de câmbio **não foram utilizados** nas camadas Silver e Gold deste MVP.\n",
    "\n",
    "**Por que coletar se não vamos usar?**\n",
    "- Demonstrar capacidade técnica de integração com múltiplas fontes\n",
    "- Preparar infraestrutura para evoluções futuras\n",
    "- As 13 perguntas de negócio definidas no OBJETIVO.md focavam em demografia/geografia, não em análises econômicas que requereriam câmbio\n",
    "\n",
    "**Oportunidades futuras:**\n",
    "- Integrar as taxas de câmbio à `dim_currencies` para análises econômicas\n",
    "- Normalizar PIB de países para USD usando essas taxas\n",
    "- Analisar volatilidade cambial ao longo do tempo\n",
    "- Criar métricas de poder de compra (ex: \"PIB per capita ajustado por paridade\")\n",
    "\n",
    "Para o escopo do MVP acadêmico, optou-se por focar nas perguntas definidas no planejamento inicial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c87274a-e7b4-4024-a85f-209f26ecab95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Passo 4: Verificando a Tabela de Câmbio\n",
    "\n",
    "**O que este código faz?**\n",
    "Mesma verificação que fizemos com países:\n",
    "1. Lê a tabela recém-criada do banco de dados\n",
    "2. Conta quantas linhas tem (esperamos 1 linha com todas as taxas)\n",
    "3. Mostra um resumo com métricas de sucesso\n",
    "\n",
    "**Confirmação de sucesso:**\n",
    "Se aparecer \"1 registro\", significa que o JSON completo com as ~160 moedas foi salvo corretamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "734b76a4-e12f-4516-8f91-7a1c804a7063",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n============================================================\nMÉTRICAS - BRONZE\n============================================================\nTabela: workspace.bronze.exchange_rates_raw\nRegistros: 4\nTimestamp: 2025-12-07 13:05:09.829069\n============================================================\n\n"
     ]
    }
   ],
   "source": [
    "# Verificar dados salvos\n",
    "df_verify_exchange = spark.table(BRONZE_TABLES['exchange_rates'])\n",
    "count_exchange = df_verify_exchange.count()\n",
    "\n",
    "log_metrics(\n",
    "    stage='bronze',\n",
    "    table_name=BRONZE_TABLES['exchange_rates'],\n",
    "    record_count=count_exchange\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8e6adb9-4ebb-41e3-871a-51cfd14db3b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Otimização das Tabelas\n",
    "\n",
    "### O que é otimização?\n",
    "Imagine que você tem uma estante com 1000 livros desorganizados. Encontrar um livro específico leva muito tempo. Se você organizar por autor e depois por título, fica muito mais rápido encontrar o que precisa. **Otimização faz isso com os dados**.\n",
    "\n",
    "### O que o comando OPTIMIZE faz?\n",
    "1. **Compactação de arquivos**: Delta Lake salva os dados em vários arquivos pequenos. OPTIMIZE junta esses arquivos em arquivos maiores e mais eficientes.\n",
    "   - **Antes**: 1000 arquivos de 1 MB cada = lento para ler\n",
    "   - **Depois**: 10 arquivos de 100 MB cada = muito mais rápido\n",
    "\n",
    "2. **Organização interna**: Rearranja os dados de forma que leituras futuras sejam mais rápidas\n",
    "\n",
    "### Por que não fazemos Z-ORDER aqui?\n",
    "Z-ORDER é outra técnica de otimização que organiza os dados por colunas específicas. **MAS** não podemos usar Z-ORDER em colunas de partição (execution_date). Como nossas tabelas são particionadas por data, não especificamos zorder_columns.\n",
    "\n",
    "### Quando isso é útil?\n",
    "- Primeira execução: Ganho pequeno (poucos dados)\n",
    "- Depois de 6 meses rodando diariamente: Ganho enorme (milhares de arquivos compactados)\n",
    "\n",
    "**Analogia:**\n",
    "É como desfragmentar um disco rígido no Windows: organiza os arquivos para que o acesso seja mais rápido.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76351338-aaed-4fb8-bc02-c5ddd867e0e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nOtimizando tabelas Bronze...\n\nOtimizando tabela workspace.bronze.countries_raw...\nTabela workspace.bronze.countries_raw otimizada com sucesso\nOtimizando tabela workspace.bronze.exchange_rates_raw...\nTabela workspace.bronze.exchange_rates_raw otimizada com sucesso\n\n✓ Otimização concluída\n"
     ]
    }
   ],
   "source": [
    "# Otimizar tabelas (sem Z-ORDER em colunas de partição)\n",
    "print(\"\\nOtimizando tabelas Bronze...\\n\")\n",
    "\n",
    "optimize_table(spark, BRONZE_TABLES['countries'])\n",
    "optimize_table(spark, BRONZE_TABLES['exchange_rates'])\n",
    "\n",
    "print(\"\\n✓ Otimização concluída\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e01f245d-362d-4835-8049-24f30fa51bf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Resumo Final da Execução\n",
    "\n",
    "### O que este código faz?\n",
    "Calcula e mostra um **relatório final** com tudo que foi feito:\n",
    "\n",
    "1. **Tempo total de execução**: Quanto tempo levou do início ao fim (diferença entre hora de início e hora atual)\n",
    "2. **Tabelas criadas**: Quais tabelas foram criadas e quantos registros cada uma tem\n",
    "3. **Status**: Se tudo deu certo (SUCESSO) ou se houve erros\n",
    "4. **Timestamp final**: Quando o processo terminou\n",
    "\n",
    "### Por que isso é importante?\n",
    "Este resumo serve para:\n",
    "- **Monitoramento**: Se o tempo aumentar muito de um dia para outro, pode indicar problemas\n",
    "- **Documentação**: Registro de que o pipeline foi executado com sucesso\n",
    "- **Auditoria**: Prova de que os dados foram coletados em determinado horário\n",
    "- **Troubleshooting**: Se algo der errado, sabemos exatamente quando e onde parou\n",
    "\n",
    "### Exemplo de output esperado:\n",
    "```\n",
    "PIPELINE BRONZE - RESUMO FINAL\n",
    "Tabelas criadas:\n",
    "  1. workspace.bronze.countries_raw (195 registros)\n",
    "  2. workspace.bronze.exchange_rates_raw (1 registro)\n",
    "Tempo total de execução: 12.45 segundos\n",
    "Status: ✓ SUCESSO\n",
    "Timestamp final: 2025-12-05 14:35:28\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "176ae438-ea76-4d5a-8794-914bca607b29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n============================================================\nPIPELINE BRONZE - RESUMO FINAL\n============================================================\n\nTabelas criadas:\n  1. workspace.bronze.countries_raw (975 registros)\n  2. workspace.bronze.exchange_rates_raw (4 registros)\n\nTempo total de execução: 28.21 segundos\nStatus: ✓ SUCESSO\nTimestamp final: 2025-12-07 13:05:17.140668\n============================================================\n"
     ]
    }
   ],
   "source": [
    "execution_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PIPELINE BRONZE - RESUMO FINAL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTabelas criadas:\")\n",
    "print(f\"  1. {BRONZE_TABLES['countries']} ({count:,} registros)\")\n",
    "print(f\"  2. {BRONZE_TABLES['exchange_rates']} ({count_exchange:,} registros)\")\n",
    "print(f\"\\nTempo total de execução: {execution_time:.2f} segundos\")\n",
    "print(f\"Status: ✓ SUCESSO\")\n",
    "print(f\"Timestamp final: {datetime.now()}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85ea39f6-7a78-4c3c-ac3c-a799247b798e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Consultas de Validação\n",
    "\n",
    "### Para que servem essas consultas?\n",
    "Agora que salvamos os dados, vamos fazer **perguntas ao banco de dados** para ter certeza de que tudo está correto. É como fazer perguntas para validar o que foi feito:\n",
    "- \"Os dados de hoje foram salvos?\"\n",
    "- \"Quantos países temos para cada data?\"\n",
    "- \"O JSON está íntegro?\"\n",
    "\n",
    "### Consulta 1: Verificar Partições\n",
    "\n",
    "**O que esta consulta faz?**\n",
    "Agrupa os dados por data de execução e mostra:\n",
    "- Quantas linhas existem para cada data\n",
    "- De qual fonte vieram (rest_countries_api)\n",
    "- Ordena da data mais recente para a mais antiga\n",
    "\n",
    "**Por que fazer isso?**\n",
    "Se rodarmos este pipeline todos os dias, vamos acumulando dados:\n",
    "- 2025-12-05: 195 países\n",
    "- 2025-12-06: 195 países\n",
    "- 2025-12-07: 195 países\n",
    "- ...\n",
    "\n",
    "Esta consulta confirma que não estamos perdendo dados no caminho e que o particionamento está funcionando.\n",
    "\n",
    "**Output esperado (primeira execução):**\n",
    "```\n",
    "+---------------+-------------+-------------------+\n",
    "|execution_date |total_records|data_source        |\n",
    "+---------------+-------------+-------------------+\n",
    "|2025-12-05     |195          |rest_countries_api |\n",
    "+---------------+-------------+-------------------+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e49307a6-09b9-4d8f-ba5b-287089cc5d66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nPartições - Countries:\n+--------------+-------------+------------------+\n|execution_date|total_records|       data_source|\n+--------------+-------------+------------------+\n|    2025-12-07|          585|rest_countries_api|\n|    2025-12-06|          390|rest_countries_api|\n+--------------+-------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Verificar partições da tabela countries\n",
    "print(\"\\nPartições - Countries:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        execution_date,\n",
    "        COUNT(*) as total_records,\n",
    "        data_source\n",
    "    FROM {BRONZE_TABLES['countries']}\n",
    "    GROUP BY execution_date, data_source\n",
    "    ORDER BY execution_date DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d24059c-2f4e-43c0-aa8c-82c8b16e49c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Consulta 2: Verificar Estrutura dos Dados\n",
    "\n",
    "**O que esta consulta faz?**\n",
    "Mostra os **primeiros 200 caracteres do JSON** de 2 países, junto com seus metadados (timestamp e data de execução).\n",
    "\n",
    "**Por que fazer isso?**\n",
    "É uma \"inspeção visual\" para confirmar que:\n",
    "1. O JSON foi salvo corretamente (não foi corrompido)\n",
    "2. Os timestamps estão corretos\n",
    "3. A estrutura do JSON está como esperado\n",
    "\n",
    "**O que esperamos ver?**\n",
    "Algo assim:\n",
    "```\n",
    "{\"name\":{\"common\":\"Brazil\",\"official\":\"Federative Republic of Brazil\"},\"tld\":[\".br\"],\"cca2\":\"BR\",\"cca3\":\"BRA\",\"population\":213993437,\"area\":8515767.0,\"capital\":[\"Brasília\"],...\n",
    "```\n",
    "\n",
    "**Por que truncar (cortar) o JSON?**\n",
    "O JSON completo de um país tem milhares de caracteres. Mostrar tudo na tela seria ilegível. Os primeiros 200 caracteres são suficientes para validar que a estrutura está correta.\n",
    "\n",
    "**LIMIT 2:**\n",
    "Mostra apenas 2 países como amostra. Não precisamos ver os 195 para validar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1992b688-1f3b-4cbe-9158-72ef1506c7f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nEstrutura dos dados (primeiros caracteres do JSON):\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------+--------------+\n|data_sample                                                                                                                                                                                             |ingestion_timestamp       |execution_date|\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------+--------------+\n|{\"name\": {\"common\": \"Antigua and Barbuda\", \"official\": \"Antigua and Barbuda\", \"nativeName\": {\"eng\": {\"official\": \"Antigua and Barbuda\", \"common\": \"Antigua and Barbuda\"}}}, \"tld\": [\".ag\"], \"cca2\": \"AG\"|2025-12-06 14:52:51.493702|2025-12-06    |\n|{\"name\": {\"common\": \"Bhutan\", \"official\": \"Kingdom of Bhutan\", \"nativeName\": {\"dzo\": {\"official\": \"\\u0f60\\u0f56\\u0fb2\\u0f74\\u0f42\\u0f0b\\u0f62\\u0f92\\u0fb1\\u0f63\\u0f0b\\u0f41\\u0f56\\u0f0b\", \"common\": \"\\u0|2025-12-06 14:52:51.493702|2025-12-06    |\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Verificar estrutura dos dados\n",
    "print(\"\\nEstrutura dos dados (primeiros caracteres do JSON):\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        SUBSTRING(data, 1, 200) as data_sample,\n",
    "        ingestion_timestamp,\n",
    "        execution_date\n",
    "    FROM {BRONZE_TABLES['countries']}\n",
    "    LIMIT 2\n",
    "\"\"\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_pipeline_bronze",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}