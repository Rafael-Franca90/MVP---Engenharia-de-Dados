{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 - Configurações e Funções Utilitárias\n",
    "## Pipeline de Dados: Bronze → Silver → Gold\n",
    "\n",
    "Este notebook contém:\n",
    "- Configurações centralizadas (APIs, schemas, tabelas)\n",
    "- Funções utilitárias reutilizáveis\n",
    "- Constantes do projeto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importação de Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime, date\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configurações de APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs das APIs\n",
    "API_URLS = {\n",
    "    'countries': 'https://restcountries.com/v3.1/independent?status=true',\n",
    "    'exchange_rates': 'https://api.exchangerate-api.com/v4/latest/USD',\n",
    "    'climate': 'https://api.open-meteo.com/v1/forecast'\n",
    "}\n",
    "\n",
    "# Configurações de request\n",
    "REQUEST_CONFIG = {\n",
    "    'timeout': 30,\n",
    "    'max_retries': 3,\n",
    "    'retry_delay': 5  # segundos\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configurações de Schemas e Tabelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schemas do Databricks\n",
    "SCHEMAS = {\n",
    "    'bronze': 'workspace.bronze',\n",
    "    'silver': 'workspace.silver',\n",
    "    'gold': 'workspace.gold'\n",
    "}\n",
    "\n",
    "# Tabelas Bronze\n",
    "BRONZE_TABLES = {\n",
    "    'countries': f\"{SCHEMAS['bronze']}.countries_raw\",\n",
    "    'exchange_rates': f\"{SCHEMAS['bronze']}.exchange_rates_raw\"\n",
    "}\n",
    "\n",
    "# Tabelas Silver\n",
    "SILVER_TABLES = {\n",
    "    'dim_countries': f\"{SCHEMAS['silver']}.dim_countries\",\n",
    "    'dim_currencies': f\"{SCHEMAS['silver']}.dim_currencies\",\n",
    "    'dim_languages': f\"{SCHEMAS['silver']}.dim_languages\",\n",
    "    'fact_country_metrics': f\"{SCHEMAS['silver']}.fact_country_metrics\"\n",
    "}\n",
    "\n",
    "# Tabelas Gold\n",
    "GOLD_TABLES = {\n",
    "    'countries_by_region': f\"{SCHEMAS['gold']}.countries_by_region\",\n",
    "    'currency_usage': f\"{SCHEMAS['gold']}.currency_usage\",\n",
    "    'language_distribution': f\"{SCHEMAS['gold']}.language_distribution\",\n",
    "    'geographic_metrics': f\"{SCHEMAS['gold']}.geographic_metrics\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Funções Utilitárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_api_data(url, timeout=30, max_retries=3):\n",
    "    \"\"\"\n",
    "    Faz requisição HTTP com retry logic\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL da API\n",
    "        timeout (int): Timeout em segundos\n",
    "        max_retries (int): Número máximo de tentativas\n",
    "    \n",
    "    Returns:\n",
    "        dict: Response JSON da API\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=timeout)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Tentativa {attempt + 1}/{max_retries} falhou: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(REQUEST_CONFIG['retry_delay'])\n",
    "            else:\n",
    "                raise Exception(f\"Falha ao acessar API após {max_retries} tentativas: {str(e)}\")\n",
    "\n",
    "def get_execution_metadata():\n",
    "    \"\"\"\n",
    "    Retorna metadados de execução\n",
    "    \n",
    "    Returns:\n",
    "        dict: Timestamp e data de execução\n",
    "    \"\"\"\n",
    "    now = datetime.now()\n",
    "    return {\n",
    "        'ingestion_timestamp': now,\n",
    "        'execution_date': now.date()\n",
    "    }\n",
    "\n",
    "def log_metrics(stage, table_name, record_count, execution_time=None):\n",
    "    \"\"\"\n",
    "    Loga métricas de execução\n",
    "    \n",
    "    Args:\n",
    "        stage (str): Nome da camada (bronze/silver/gold)\n",
    "        table_name (str): Nome da tabela\n",
    "        record_count (int): Número de registros\n",
    "        execution_time (float): Tempo de execução em segundos\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MÉTRICAS - {stage.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Tabela: {table_name}\")\n",
    "    print(f\"Registros: {record_count:,}\")\n",
    "    if execution_time:\n",
    "        print(f\"Tempo de execução: {execution_time:.2f} segundos\")\n",
    "    print(f\"Timestamp: {datetime.now()}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "def create_database_if_not_exists(spark, schema_name):\n",
    "    \"\"\"\n",
    "    Cria database se não existir\n",
    "    \n",
    "    Args:\n",
    "        spark: SparkSession\n",
    "        schema_name (str): Nome do schema\n",
    "    \"\"\"\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {schema_name}\")\n",
    "    print(f\"Database '{schema_name}' verificado/criado com sucesso\")\n",
    "\n",
    "def optimize_table(spark, table_name, zorder_columns=None):\n",
    "    \"\"\"\n",
    "    Otimiza tabela Delta com OPTIMIZE e Z-ORDER\n",
    "    \n",
    "    Args:\n",
    "        spark: SparkSession\n",
    "        table_name (str): Nome completo da tabela\n",
    "        zorder_columns (list): Colunas para Z-ORDER\n",
    "    \"\"\"\n",
    "    print(f\"Otimizando tabela {table_name}...\")\n",
    "    \n",
    "    if zorder_columns:\n",
    "        zorder_cols = ', '.join(zorder_columns)\n",
    "        spark.sql(f\"OPTIMIZE {table_name} ZORDER BY ({zorder_cols})\")\n",
    "    else:\n",
    "        spark.sql(f\"OPTIMIZE {table_name}\")\n",
    "    \n",
    "    print(f\"Tabela {table_name} otimizada com sucesso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inicialização dos Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar schemas se não existirem\n",
    "def initialize_schemas(spark):\n",
    "    \"\"\"\n",
    "    Inicializa todos os schemas necessários\n",
    "    \"\"\"\n",
    "    for schema in SCHEMAS.values():\n",
    "        create_database_if_not_exists(spark, schema)\n",
    "    print(\"\\nTodos os schemas foram inicializados!\")\n",
    "\n",
    "# Descomente para executar a inicialização\n",
    "# initialize_schemas(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Teste de Conectividade das APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_api_connectivity():\n",
    "    \"\"\"\n",
    "    Testa conectividade com todas as APIs configuradas\n",
    "    \"\"\"\n",
    "    print(\"Testando conectividade das APIs...\\n\")\n",
    "    \n",
    "    for api_name, url in API_URLS.items():\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            status = \"✓ OK\" if response.status_code == 200 else f\"✗ ERRO ({response.status_code})\"\n",
    "            print(f\"{api_name:20s}: {status}\")\n",
    "        except Exception as e:\n",
    "            print(f\"{api_name:20s}: ✗ FALHA - {str(e)}\")\n",
    "    \n",
    "    print(\"\\nTeste de conectividade finalizado!\")\n",
    "\n",
    "# Descomente para executar o teste\n",
    "# test_api_connectivity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Configurações de Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configurações do Spark (compatível com Databricks Runtime 4.0.0)\n# Habilitar Adaptive Query Execution se disponível\ntry:\n    spark.conf.set(\"spark.sql.adaptive.enabled\", True)\n    print(\"✓ Adaptive Query Execution habilitado\")\nexcept:\n    print(\"⚠ Adaptive Query Execution não disponível nesta versão\")\n\nprint(\"\\nConfigurações carregadas com sucesso!\")\nprint(f\"\\nSchemas configurados:\")\nfor key, value in SCHEMAS.items():\n    print(f\"  - {key}: {value}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}